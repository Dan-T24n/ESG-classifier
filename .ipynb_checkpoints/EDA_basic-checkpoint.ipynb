{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install PyMuPDF\n",
    "!pip install nltk\n",
    "!pip install ocrmypdf\n",
    "!pip install pytesseract\n",
    "!pip install pngquant\n",
    "!pip install pyarrow \n",
    "!pip install fastparquet\n",
    "!pip install datasets\n",
    "!pip install icecream\n",
    "!pip install fastparquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T15:45:06.273642Z",
     "iopub.status.busy": "2023-06-02T15:45:06.273223Z",
     "iopub.status.idle": "2023-06-02T15:45:06.282024Z",
     "shell.execute_reply": "2023-06-02T15:45:06.281125Z",
     "shell.execute_reply.started": "2023-06-02T15:45:06.273601Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd \n",
    "import fitz \n",
    "from PIL import Image\n",
    "import re\n",
    "import pngquant\n",
    "from unidecode import unidecode\n",
    "import torch\n",
    "import evaluate\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T15:45:06.283674Z",
     "iopub.status.busy": "2023-06-02T15:45:06.283375Z",
     "iopub.status.idle": "2023-06-02T15:45:06.299613Z",
     "shell.execute_reply": "2023-06-02T15:45:06.298548Z",
     "shell.execute_reply.started": "2023-06-02T15:45:06.283648Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_pdf(path):\n",
    "    '''Reads a PDF file and returns a PdfDocument object.'''\n",
    "    doc = fitz.open(path)\n",
    "    return doc\n",
    "\n",
    "def render(page):\n",
    "    '''Render a page to a PIL image.'''\n",
    "    bitmap = page.render(\n",
    "        scale = 1,    # 72dpi resolution\n",
    "        rotation = 0, # no additional rotation\n",
    "    )\n",
    "    return bitmap.to_pil()\n",
    "\n",
    "# cleanup content using regex:\n",
    "def clean_characters(text):\n",
    "    text = unidecode(text) # convert to ascii\n",
    "    text = re.sub(r\"[^a-zA-Z0-9.£$€']+\", ' ', text) # strategy 1: keep numbers\n",
    "    # text = re.sub(r\"[^a-zA-Z:.£$€'\\n]+\", ' ', text) ## strategy 2: remove numbers and %\n",
    "    text = ' '.join([w for w in text.split() if len(w)>1] )\n",
    "    return text\n",
    "\n",
    "def extract_content_from_id(file_id: str) -> str :    \n",
    "    # extract filename and page\n",
    "    items = file_id.split('.')\n",
    "    filename = '.'.join(items[:2])\n",
    "    page_num = int(items[-1])-1\n",
    "    \n",
    "    # load pdf, select page, and extract its content\n",
    "    filepath = os.path.join(report_path, filename) #path to the report\n",
    "    doc = get_pdf(filepath) # load the pdf\n",
    "    page = doc.load_page(page_num) # select the page from the pdf\n",
    "    content = page.get_text(\"text\", sort = True, flags=fitz.TEXT_INHIBIT_SPACES) # extract the text from the page\n",
    "    content = unidecode(content) # convert to ascii\n",
    "    content = clean_characters(content) # replace unrecognized characters\n",
    "    return content\n",
    "\n",
    "def render_content_from_id(file_id: str) -> str :    \n",
    "    # extract filename and page\n",
    "    items = file_id.split('.')\n",
    "    filename = '.'.join(items[:2]) # reconstruct the report name\n",
    "    page_num = int(items[-1])-1\n",
    "    \n",
    "    # load pdf, select page, and extract its content\n",
    "    filepath = os.path.join(report_path, filename) #path to the report\n",
    "    doc = get_pdf(filepath) # load the report pdf\n",
    "    dpi = 150  # set the dpi\n",
    "    mat = fitz.Matrix(dpi / 72, dpi / 72)  # sets zoom factor\n",
    "    pix = doc[page_num].get_pixmap(matrix=mat)\n",
    "    img_page = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "    return img_page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set path for folder\n",
    "\n",
    "- choose one of the following blocks: online if run directly on Kaggle, local if data downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # kaggle online path\n",
    "# basepath = \"/kaggle/input/oxml2023mlcases-esg-classifier/data/\"\n",
    "# report_path = os.path.join(basepath, \"reports\")\n",
    "# label_path = os.path.join(basepath, \"labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local data path \n",
    "\n",
    "basepath = '/Users/mbp14/Desktop/GoogleDrive/OxML_2023/code/Kaggle'\n",
    "\n",
    "# build get path function, input folder name (as a list) and file name (optional), return the path\n",
    "def get_path(folder_name: list, file_name=None):\n",
    "    path = Path(basepath)\n",
    "    for folder in folder_name:\n",
    "        path = path / folder\n",
    "    if file_name:\n",
    "        path = path / file_name\n",
    "    return path\n",
    "\n",
    "# report path\n",
    "report_path = get_path([\"data\", \"reports\"])\n",
    "\n",
    "# label path\n",
    "label_path = get_path([\"data\"], \"labels.csv\")\n",
    "\n",
    "# output path\n",
    "output_path = get_path([], \"output\")\n",
    "\n",
    "# submit sample path\n",
    "submit_path = get_path([], \"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report path:  /Users/mbp14/Desktop/GoogleDrive/OxML_2023/code/Kaggle/data/reports\n",
      "Label path:  /Users/mbp14/Desktop/GoogleDrive/OxML_2023/code/Kaggle/data/labels.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Report path: \", report_path)\n",
    "print(\"Label path: \", label_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated id:  5\n"
     ]
    }
   ],
   "source": [
    "# read label file to get id and class\n",
    "df = pd.read_csv(label_path)\n",
    "\n",
    "# check duplicates id\n",
    "print(\"Number of duplicated id: \", df.duplicated(subset=['id']).sum())\n",
    "\n",
    "# slice duplicated id, all entries with duplicated id\n",
    "df_duplicate = df[df.duplicated(subset=['id'], keep=False)].sort_values(by=['id'])\n",
    "df_duplicate # duplicates have conflicting labels, due to nature of the pages\n",
    "\n",
    "# drop the duplicates (keep the first entry)\n",
    "df = df.drop_duplicates(subset=['id'], keep='first').sort_values(by=['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract content from pdf pages\n",
    "\n",
    "- for each row, we grab the 'id', which contains a report_id and a page number\n",
    "- use a function to load the correct page from the pdf report, and extract the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unidecode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m test_id \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39miloc[\u001b[39m4\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[39m# extract content from the pdf page\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m test_content \u001b[39m=\u001b[39m extract_content_from_id(test_id)\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(test_content)\n\u001b[1;32m      8\u001b[0m \u001b[39m# render the page\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 33\u001b[0m, in \u001b[0;36mextract_content_from_id\u001b[0;34m(file_id)\u001b[0m\n\u001b[1;32m     31\u001b[0m page \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39mload_page(page_num) \u001b[39m# select the page from the pdf\u001b[39;00m\n\u001b[1;32m     32\u001b[0m content \u001b[39m=\u001b[39m page\u001b[39m.\u001b[39mget_text(\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m, sort \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m, flags\u001b[39m=\u001b[39mfitz\u001b[39m.\u001b[39mTEXT_INHIBIT_SPACES) \u001b[39m# extract the text from the page\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m content \u001b[39m=\u001b[39m unidecode(content) \u001b[39m# convert to ascii\u001b[39;00m\n\u001b[1;32m     34\u001b[0m content \u001b[39m=\u001b[39m clean_characters(content) \u001b[39m# replace unrecognized characters\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39mreturn\u001b[39;00m content\n",
      "\u001b[0;31mNameError\u001b[0m: name 'unidecode' is not defined"
     ]
    }
   ],
   "source": [
    "# test extract function\n",
    "test_id = df.iloc[4]['id']\n",
    "\n",
    "# extract content from the pdf page\n",
    "test_content = extract_content_from_id(test_id)\n",
    "print(test_content)\n",
    "\n",
    "# render the page\n",
    "render_content_from_id(test_id)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T15:45:06.485821Z",
     "iopub.status.busy": "2023-06-02T15:45:06.485377Z",
     "iopub.status.idle": "2023-06-02T15:45:51.101406Z",
     "shell.execute_reply": "2023-06-02T15:45:51.100569Z",
     "shell.execute_reply.started": "2023-06-02T15:45:06.48578Z"
    }
   },
   "outputs": [],
   "source": [
    "contents = []\n",
    "\n",
    "# iterate over each row and extract content\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    # get file id\n",
    "    file_id = row['id']\n",
    "    # extract content from \n",
    "    content = extract_content_from_id(file_id)\n",
    "    contents.append(content)\n",
    "    \n",
    "df['content'] = contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T15:45:51.106951Z",
     "iopub.status.busy": "2023-06-02T15:45:51.106574Z",
     "iopub.status.idle": "2023-06-02T15:45:51.119965Z",
     "shell.execute_reply": "2023-06-02T15:45:51.119Z",
     "shell.execute_reply.started": "2023-06-02T15:45:51.106919Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column with the report name\n",
    "df['report_name'] = df['id'].apply(lambda x: x.split('.')[0])\n",
    "\n",
    "# creat a new column with the page number\n",
    "df['page_num'] = df['id'].apply(lambda x: x.split('.')[-1])\n",
    "\n",
    "# reorder columns\n",
    "df = df[['id','report_name', 'page_num', 'class', 'content']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df locally to parquet\n",
    "df.to_parquet(get_path([\"process\"], \"data_v0.parquet\"))\n",
    "\n",
    "# save df locally to csv\n",
    "df.to_csv(get_path([\"process\"], \"data_v0.parquet\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of unique reports\n",
    "num_report = df['report_name'].nunique()\n",
    "ic(num_report)\n",
    "\n",
    "# find the median number of pages per report\n",
    "median_page = df.groupby('report_name')['page_num'].nunique().median()\n",
    "ic(median_page)\n",
    "\n",
    "# find the 75th percentile of pages per report\n",
    "percentile75_page = df.groupby('report_name')['page_num'].nunique().quantile(0.75)\n",
    "ic(percentile75_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot the number of unique pages in each report using bar plot, order by number of pages\n",
    "num_pages = df.groupby('report_name')['page_num'].nunique().sort_values(ascending=False)\n",
    "\n",
    "# set the figure size, x and y axis labels, and plot the bar plot, tidy up the plot labels and title\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.xlabel('Report Name')\n",
    "plt.ylabel('Number of Pages')\n",
    "plt.bar(num_pages.index, num_pages.values)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Number of Pages in Each Report')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plot the number of unique pages in each report using histogram\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.xlabel('Number of Pages')\n",
    "plt.ylabel('Number of Reports')\n",
    "plt.hist(num_pages.values, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary for color mapping of each class\n",
    "color_dict = {'environmental': 'green', 'governance': 'orange','social': 'red', 'other': 'blue'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by classes and count the number of reports in each class\n",
    "report_count = df.groupby('class')['report_name'].nunique()\n",
    "\n",
    "# plot number of reports in each class using bar plot, different colors for each class\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Reports')\n",
    "report_count.plot(kind='bar', color=[color_dict[i] for i in report_count.index])  \n",
    "plt.title('Number of Reports in Each Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot number of pages in each class using bar plot, different colors for each class\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Pages')\n",
    "df.groupby('class')['page_num'].nunique().plot(kind='bar', color=[color_dict[i] for i in report_count.index])  \n",
    "plt.title('Number of Pages in Each Class')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by report name, and count the number of pages in each class\n",
    "num_pages_class = df.groupby('report_name')['class'].value_counts().unstack()\n",
    "\n",
    "# plot a bar chart, highlight different classes with different colors within each bar\n",
    "colors = ['green', 'orange', 'blue', 'red']\n",
    "num_pages_class.plot(kind='bar', stacked=True, figsize=(20,5), color  = color_dict)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Report Name')\n",
    "plt.ylabel('Number of Pages')\n",
    "plt.title('Number of Pages in Each Report by Class')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights at document-level\n",
    "- 47 reports in total, 75% have less than 50 pages\n",
    "- reports with more pages tend to have label 'other' (maybe artefact or training data)\n",
    "- class equally distributed by report (a report can have multiple classes -E,-S,-G), except 'other' (is it a rule?)\n",
    "- by pages: classes are skew, many 'other'. While the E/S/G classes are roughly equal\n",
    "- by pages: total 'other' is roughly equal sum of all other classes\n",
    "\n",
    "Issue: strategies may differ for document-level or page-level classification, 2 possible testing\n",
    "- pages randomly drawed from a set of reports (rare to have 2 pages from same report)\n",
    "- draw reports and classify all included pages (likely to have multiple pages from same reports)\n",
    "\n",
    "Thoughts:\n",
    "- should we learn document meta-features? probably not, have to be flexible at page-level\n",
    "- need stratify when splitting (preserve classes distribution by pages/reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize content for EDA\n",
    "\n",
    "- number of tokens on each page\n",
    "- class distribution by number of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build text processing functions\n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import string\n",
    "import re\n",
    "\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a function to preprocess text: tokenize, remove stopwords and punctuation, lemmatize\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "def process_text(text):\n",
    "  ''' function to perform text preprocessing for EDA, not for modelling purpose\n",
    "  input: text\n",
    "  output: list of word tokens\n",
    "  '''\n",
    "  \n",
    "  # Tokenize the text\n",
    "  tokens = word_tokenize(text.lower())\n",
    "  \n",
    "  # filter stopwords and punctuation\n",
    "  tokens = [token for token in tokens if token not in stopwords.words('english') and token not in string.punctuation]\n",
    "\n",
    "  # Lemmatize words\n",
    "  tokens = [lem.lemmatize(token, \"v\") for token in tokens]\n",
    "  \n",
    "  # # stemming: reduce words to their root form, may lead to loss of context in small dataset \n",
    "  # # example: we want to keep sustain, sustainability, sustainable, under stemming they are the same\n",
    "  # stemmer = PorterStemmer()\n",
    "  # tokens = [stemmer.stem(token) for token in tokens]\n",
    "  \n",
    "  # Remove special characters and numbers from tokens\n",
    "  tokens = [re.sub(r\"[^a-zA-Z]+\", '', token) for token in tokens]\n",
    "  \n",
    "  # Remove empty strings\n",
    "  tokens = list(filter(None, tokens))\n",
    "\n",
    "  # Remove token that are less than 1 character: cleaning up\n",
    "  tokens = [token for token in tokens if len(token) > 1]\n",
    "\n",
    "  return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the function on test content\n",
    "test_tokens = process_text(test_content)\n",
    "test_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the dataframe\n",
    "df_eda = df.copy()\n",
    "\n",
    "# apply the function to the whole dataframe on the content column\n",
    "df_eda['tokens'] = df_eda['content'].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the content column\n",
    "df_eda.drop('content', axis=1, inplace=True)\n",
    "\n",
    "# add a new column: count the number of tokens in each row\n",
    "df_eda['num_tokens'] = df_eda['tokens'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe the distribution of number of tokens\n",
    "df_eda['num_tokens'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all tokens into a list\n",
    "all_tokens = []\n",
    "for tokens in df_eda['tokens']:\n",
    "    all_tokens += tokens\n",
    "    \n",
    "# make the list of unique tokens\n",
    "unique_tokens = list(set(all_tokens))\n",
    "\n",
    "# describe the distribution of character length of unique tokens\n",
    "unique_tokens_len = [len(token) for token in unique_tokens]\n",
    "\n",
    "# concatenate unique tokens and their length into a dataframe\n",
    "df_unique_tokens = pd.DataFrame({'token': unique_tokens, 'length': unique_tokens_len})\n",
    "\n",
    "# describe the distribution of character length of unique tokens\n",
    "df_unique_tokens.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of character length of unique tokens using boxplot\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.xlabel('Character Length')\n",
    "plt.boxplot(df_unique_tokens['length'])\n",
    "plt.title('Distribution of Character Length of Unique Tokens')\n",
    "plt.show()\n",
    "\n",
    "# around 15 characters: indicative of parsing error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights from tokenization\n",
    "\n",
    "- some phrases are not parsed correctly: tokenizer or PDF parsing error\n",
    "- PDF parsing is most important, then tokenizer\n",
    "- text pre-processing trade-off, find balance\n",
    "\n",
    "Strategies:\n",
    "- sentent tokenizer then word tokenizer\n",
    "- change order: clean up text first then tokenize\n",
    "- try Keras and different tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df to local parquet file\n",
    "df_eda.to_parquet(os.path.join(basepath, \"process\", \"data_v1.parquet\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
