{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install PyMuPDF\n",
    "# !pip install -U nltk\n",
    "# !pip install ocrmypdf -U\n",
    "# !brew install tesseract\n",
    "# !pip install pytesseract\n",
    "# !pip install pngquant\n",
    "# !pip install pyarrow \n",
    "# !pip install fastparquet\n",
    "# !brew install cmake\n",
    "# !brew install pkg-config\n",
    "# !pip install transformers\n",
    "# !pip install datasets\n",
    "# !pip install einops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T15:45:06.273642Z",
     "iopub.status.busy": "2023-06-02T15:45:06.273223Z",
     "iopub.status.idle": "2023-06-02T15:45:06.282024Z",
     "shell.execute_reply": "2023-06-02T15:45:06.281125Z",
     "shell.execute_reply.started": "2023-06-02T15:45:06.273601Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd \n",
    "from icecream import ic\n",
    "import fitz \n",
    "from PIL import Image\n",
    "import re\n",
    "import pngquant\n",
    "from unidecode import unidecode \n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import pipeline\n",
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "from transformers import logging\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# from bertopic import BERTopic\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T15:45:06.283674Z",
     "iopub.status.busy": "2023-06-02T15:45:06.283375Z",
     "iopub.status.idle": "2023-06-02T15:45:06.299613Z",
     "shell.execute_reply": "2023-06-02T15:45:06.298548Z",
     "shell.execute_reply.started": "2023-06-02T15:45:06.283648Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_pdf(path):\n",
    "    '''Reads a PDF file and returns a PdfDocument object.'''\n",
    "    doc = fitz.open(path)\n",
    "    return doc\n",
    "\n",
    "def render(page):\n",
    "    '''Render a page to a PIL image.'''\n",
    "    bitmap = page.render(\n",
    "        scale = 1,    # 72dpi resolution\n",
    "        rotation = 0, # no additional rotation\n",
    "    )\n",
    "    return bitmap.to_pil()\n",
    "\n",
    "# cleanup content using regex:\n",
    "def clean_characters(text):\n",
    "    text = unidecode(text) # convert to ascii\n",
    "    text = re.sub(r\"[^a-zA-Z0-9.£$€'\\n]+\", ' ', text) # strategy 1: keep numbers\n",
    "    # text = re.sub(r\"[^a-zA-Z:.£$€'\\n]+\", ' ', text) ## strategy 2: remove numbers and %\n",
    "    clean_text = ' '.join( [w for w in text.split() if len(w)>1] )\n",
    "    return text\n",
    "\n",
    "def extract_content_from_id(file_id: str) -> str :    \n",
    "    # extract filename and page\n",
    "    items = file_id.split('.')\n",
    "    filename = '.'.join(items[:2])\n",
    "    page_num = int(items[-1])-1\n",
    "    \n",
    "    # load pdf, select page, and extract its content\n",
    "    filepath = os.path.join(report_path, filename) #path to the report\n",
    "    doc = get_pdf(filepath) # load the pdf\n",
    "    page = doc.load_page(page_num) # select the page from the pdf\n",
    "    content = page.get_text(\"text\", sort = True, flags=fitz.TEXT_INHIBIT_SPACES) # extract the text from the page\n",
    "    content = clean_characters(content) # replace unrecognized characters\n",
    "    return content\n",
    "\n",
    "def render_content_from_id(file_id: str) -> str :    \n",
    "    # extract filename and page\n",
    "    items = file_id.split('.')\n",
    "    filename = '.'.join(items[:2]) # reconstruct the report name\n",
    "    page_num = int(items[-1])-1\n",
    "    \n",
    "    # load pdf, select page, and extract its content\n",
    "    filepath = os.path.join(report_path, filename) #path to the report\n",
    "    doc = get_pdf(filepath) # load the report pdf\n",
    "    dpi = 150  # set the dpi\n",
    "    mat = fitz.Matrix(dpi / 72, dpi / 72)  # sets zoom factor\n",
    "    pix = doc[page_num].get_pixmap(matrix=mat)\n",
    "    img_page = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "    return img_page"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set path for folder\n",
    "\n",
    "- choose one of the following blocks: online if run directly on Kaggle, local if data downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local data path using pathlib\n",
    "\n",
    "basepath = '/Users/mbp14/Desktop/GoogleDrive/OxML_2023/code/Kaggle'\n",
    "\n",
    "# build get path function, input folder name (as a list) and file name (optional), return the path\n",
    "def get_path(folder_name: list, file_name=None):\n",
    "    path = Path(basepath)\n",
    "    for folder in folder_name:\n",
    "        path = path / folder\n",
    "    if file_name:\n",
    "        path = path / file_name\n",
    "    return path\n",
    "\n",
    "# report path\n",
    "report_path = get_path([\"data\", \"reports\"])\n",
    "\n",
    "# label path\n",
    "label_path = get_path([\"data\"], \"labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report path:  /Users/mbp14/Desktop/GoogleDrive/OxML_2023/code/Kaggle/data/reports\n",
      "Label path:  /Users/mbp14/Desktop/GoogleDrive/OxML_2023/code/Kaggle/data/labels.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Report path: \", report_path)\n",
    "print(\"Label path: \", label_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read label file to get id and class\n",
    "# df = pd.read_csv(label_path)\n",
    "\n",
    "# # check duplicates id\n",
    "# print(\"Number of duplicated id: \", df.duplicated(subset=['id']).sum())\n",
    "\n",
    "# # slice duplicated id, all entries with duplicated id\n",
    "# df_duplicate = df[df.duplicated(subset=['id'], keep=False)].sort_values(by=['id'])\n",
    "# df_duplicate # duplicates have conflicting labels, due to nature of the pages\n",
    "\n",
    "# # drop the duplicates (keep the first entry)\n",
    "# df = df.drop_duplicates(subset=['id'], keep='first').sort_values(by=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # extract content from each page\n",
    "# contents = []\n",
    "\n",
    "# for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "#     # get file id\n",
    "#     file_id = row['id']\n",
    "#     # extract content from \n",
    "#     content = extract_content_from_id(file_id)\n",
    "#     contents.append(content)\n",
    "    \n",
    "# df['content'] = contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # create a new column with the report name\n",
    "# df['report_name'] = df['id'].apply(lambda x: x.split('.')[0])\n",
    "\n",
    "# # creat a new column with the page number\n",
    "# df['page_num'] = df['id'].apply(lambda x: x.split('.')[-1])\n",
    "\n",
    "# # reorder columns\n",
    "# df = df[['id','report_name', 'page_num', 'class', 'content']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dictionary mapping class label to numeric\n",
    "# map_label = {'other': 0, 'environmental': 1, 'social': 2, 'governance': 3}\n",
    "\n",
    "# # use the dictionary to replace the class label\n",
    "# df['target'] = df['class'].map(map_label)\n",
    "\n",
    "# # save df to parquet\n",
    "# df.to_parquet(get_path([\"process\"], \"data_v2.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/mbp14/Desktop/GoogleDrive/OxML_2023/code/Kaggle/process/data_v2.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# reload df from parquet\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprocess\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata_v2.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.9/site-packages/pandas/io/parquet.py:503\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;124;03mLoad a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;124;03mDataFrame\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    501\u001b[0m impl \u001b[38;5;241m=\u001b[39m get_engine(engine)\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.9/site-packages/pandas/io/parquet.py:244\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    242\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m--> 244\u001b[0m path_or_handle, handles, kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilesystem\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfilesystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    251\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[1;32m    252\u001b[0m         path_or_handle, columns\u001b[38;5;241m=\u001b[39mcolumns, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    253\u001b[0m     )\u001b[38;5;241m.\u001b[39mto_pandas(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mto_pandas_kwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.9/site-packages/pandas/io/parquet.py:102\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m     92\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.9/site-packages/pandas/io/common.py:865\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    868\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/mbp14/Desktop/GoogleDrive/OxML_2023/code/Kaggle/process/data_v2.parquet'"
     ]
    }
   ],
   "source": [
    "# reload df from parquet\n",
    "df = pd.read_parquet(get_path([\"process\"], \"data_v2.parquet\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction test\n",
    "\n",
    "- test tokenizer\n",
    "- test embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a sample of the df to test the model\n",
    "df_sample = df.sample(100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of models\n",
    "finbert_esg = 'yiyanghkust/finbert-esg'\n",
    "sentence_roberta = 'sentence-transformers/all-distilroberta-v1'\n",
    "fin_roberta = 'soleimanian/financial-roberta-large-sentiment'\n",
    "bart = 'facebook/bart-large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df_sample)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(finbert_esg)\n",
    "model = AutoModel.from_pretrained(finbert_esg)\n",
    "\n",
    "# check content\n",
    "test_content = dataset[0]['content']\n",
    "print(f'content: {test_content} *** label: {dataset[0][\"class\"]}')\n",
    "\n",
    "# test the tokenizer\n",
    "tokens = tokenizer(test_content, truncation=True, padding='max_length', max_length=512, return_tensors='pt')\n",
    "\n",
    "# size of the tokens\n",
    "tokens.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the tokenized sentence with the model\n",
    "outputs = model(**tokens)\n",
    "\n",
    "# content of the output\n",
    "print(outputs.last_hidden_state.shape)\n",
    "\n",
    "# check output keys\n",
    "print(outputs.keys())\n",
    "\n",
    "# last hidden state \n",
    "outputs.last_hidden_state\n",
    "\n",
    "# # extract the logits as array\n",
    "# logits = outputs.logits.detach().numpy()\n",
    "\n",
    "# # check value of logits: only valid for classification models\n",
    "# logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extractor pipeline\n",
    "# extract = pipeline(\"feature-extraction\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# # test the pipeline\n",
    "# feature = extract(test_content, truncation=True, padding = 'max_lenght', return_tensors='pt', max_length=512)\n",
    "\n",
    "# # Warnings: some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
    "# # because no fine-tuning yet\n",
    "\n",
    "# # check size and shape of the feature\n",
    "# print(feature.shape)\n",
    "# feature\n",
    "\n",
    "# extracted feature depends on the type of model (classifier), not just the task\n",
    "# some models return logits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skip\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "# model name\n",
    "model_name = bart\n",
    "\n",
    "#Sentences we want sentence embeddings for\n",
    "sentences = test_content\n",
    "\n",
    "#Load AutoModel from huggingface model repository\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True )\n",
    "\n",
    "#Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=\"max_length\", truncation=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "#Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input, output_hidden_states=True)\n",
    "\n",
    "\n",
    "# #Perform pooling. In this case, mean pooling\n",
    "# sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skip\n",
    "\n",
    "# check model output\n",
    "print(model_output.keys())\n",
    "\n",
    "# check shape of the model output\n",
    "hidden = model_output[0]\n",
    "\n",
    "print('size of hidden layers:', len(hidden))\n",
    "\n",
    "# convert hidden tuple to array and check shape: tuple (nubmer_layers, batch_size, sequence_length, hidden_size)\n",
    "hidden_array = torch.stack(hidden)\n",
    "print(hidden_array.shape)\n",
    "\n",
    "# extract embeddings from hidden layer: last hidden state = last layer from hidden\n",
    "emb_last_hidden_state = model_output['last_hidden_state']\n",
    "emb_last_layer = model_output['hidden_states'][-1] # not always work\n",
    "\n",
    "# check whether the last hidden state is the same as the last layer\n",
    "emb_last_layer == emb_last_hidden_state\n",
    "\n",
    "# check shape of the embeddings\n",
    "print(emb_last_hidden_state.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model for embedding\n",
    "\n",
    "- select a list of model\n",
    "- extract embedding given one example\n",
    "- check how different the embeddings are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get example content\n",
    "example_content = dataset[3]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all models to a list\n",
    "models = [finbert_esg, sentence_roberta, fin_roberta, bart]\n",
    "\n",
    "# create list of model names\n",
    "model_names = ['finbert_esg', 'sentence_roberta', 'fin_roberta']\n",
    "\n",
    "# zip to make a dictionary\n",
    "model_dict = dict(zip(model_names, models))\n",
    "\n",
    "# loop through the models, print name and value\n",
    "for name, model in model_dict.items():\n",
    "    print(name, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(last_hidden, attention_mask):\n",
    "     '''input: hidden state tensor (batch_size, sequence_length, hidden_size) and attention mask (batch_size, sequence_length) \n",
    "     output: the mean of the hidden state (batch_size, hidden_size)\n",
    "     attention mask is used to ignore the padding tokens'''\n",
    "     token_embeddings = last_hidden # hidden state tensor (batch_size, sequence_length, hidden_size)\n",
    "     input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "     sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "     sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "     return sum_embeddings / sum_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skip\n",
    "\n",
    "# define extraction workflow\n",
    "\n",
    "# set model\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "# set tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#Tokenize text\n",
    "encoded_input = tokenizer(example_content, padding=\"max_length\", truncation=True, max_length=512, return_tensors='pt')\n",
    "#Compute model output\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input, output_hidden_states=True)\n",
    "\n",
    "# extract embeddings: last hidden state, a tensor of shape (batch_size, sequence_length, hidden_size)\n",
    "last_hidden = model_output['last_hidden_state']\n",
    "attention_mask = encoded_input['attention_mask']\n",
    "\n",
    "#Perform mean pooling, using attention mask to take into account padding\n",
    "text_embeddings = mean_pooling(last_hidden,attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build function for feature extraction workflow\n",
    "def test_feature_extraction(model_name, content):\n",
    "    '''input: model name and content\n",
    "    output: extracted feature'''\n",
    "    # set model\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    # set tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    #Tokenize text\n",
    "    encoded_input = tokenizer(content, padding=\"max_length\", truncation=True, max_length=512, return_tensors='pt')\n",
    "    #Compute model output\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input, output_hidden_states=True)\n",
    "    # extract embeddings: last hidden state, a tensor of shape (batch_size, sequence_length, hidden_size)\n",
    "    last_hidden = model_output['last_hidden_state']\n",
    "    attention_mask = encoded_input['attention_mask']\n",
    "    #Perform mean pooling, using attention mask to take into account padding\n",
    "    text_embeddings = mean_pooling(last_hidden,attention_mask)\n",
    "    return text_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skip\n",
    "\n",
    "# make empty dictionary for storing extracted features\n",
    "extracted_features = {}\n",
    "\n",
    "# loop through models in model_dict and extract features\n",
    "for name, model in model_dict.items():\n",
    "    # extract feature\n",
    "    feature = test_feature_extraction(model, example_content)\n",
    "    # add feature to dictionary\n",
    "    extracted_features[name] = feature\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skip\n",
    "\n",
    "# check extracted features\n",
    "print(extracted_features.keys())\n",
    "\n",
    "# loop through extracted features and print size of each feature\n",
    "for name, feature in extracted_features.items():\n",
    "    print(name, feature.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loop through the extracted features and convert each feature to a numpy array\n",
    "# for name, feature in extracted_features.items():\n",
    "#     extracted_features[name] = feature.numpy().flatten()\n",
    "#     print(name, type(feature))\n",
    "#     print(name, feature.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # check the dataframe\n",
    "# print(extracted_features['finbert_esg'].shape)\n",
    "\n",
    "# # flatten() makes the whole array look like a list\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch test\n",
    "\n",
    "Strategy 1: iterate df\n",
    "- each row: get content, tokenize, embed\n",
    "- save in dictionary\n",
    "\n",
    "Strategy 2: ds and batch tokenizer - better for fine-tuning\n",
    "- convert all df to ds\n",
    "- tokenize in batch with ds\n",
    "- use embedding with encoded_ds\n",
    "- convert embedding back to df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build function for feature extraction workflow\n",
    "def feature_extraction(model_name, content):\n",
    "    '''input: model name and content\n",
    "    output: extracted feature'''\n",
    "    # set model\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    # set tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    #Tokenize text\n",
    "    encoded_input = tokenizer(content, padding=\"max_length\", truncation=True, max_length=512, return_tensors='pt')\n",
    "    #Compute model output\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input, output_hidden_states=True)\n",
    "    # extract embeddings: last hidden state, a tensor of shape (batch_size, sequence_length, hidden_size)\n",
    "    last_hidden = model_output['last_hidden_state']\n",
    "    attention_mask = encoded_input['attention_mask']\n",
    "    #Perform mean pooling, using attention mask to take into account padding\n",
    "    text_embeddings = mean_pooling(last_hidden,attention_mask)\n",
    "    return text_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build iterator function to return content and id for every row\n",
    "def content_iterator(df):\n",
    "    '''input: dataframe\n",
    "    output: content for every row'''\n",
    "    for index, row in df.iterrows():\n",
    "        yield row['content'], row['id']\n",
    "        \n",
    "# test the iterator function to print\n",
    "for content,id in content_iterator(df_sample):\n",
    "    print(f'file id: {id} \\n*** content ***\\n {content}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make empty dictionary for storing extracted features\n",
    "extracted_features = {}\n",
    "\n",
    "# set model name\n",
    "model_name = sentence_roberta\n",
    "\n",
    "# use iterator function and extract function to extract feature for each row\n",
    "for content,id in content_iterator(df_sample):\n",
    "    feature = feature_extraction(model_name, content) # extract feature\n",
    "    feature = feature.numpy().flatten() # convert feature tensor to numpy array\n",
    "    extracted_features[id] = feature # append feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check shape of extracted features: 2-level dictionary (num_row, feature_size), each row is a feature\n",
    "print(len(extracted_features))\n",
    "\n",
    "# convert extracted features to a dataframe, each row is a row, each column is one dimension of the feature\n",
    "df_features = pd.DataFrame(extracted_features).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of df feature\n",
    "df_features_label = df_features.copy()\n",
    "\n",
    "# use id column to fetch class from df_sample\n",
    "df_features_label['class'] = df_sample.set_index('id')['class']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract feature workflow\n",
    "\n",
    "`Sentence_transformer`: easier and straightforward\n",
    "- sentence_transformer is compatible with any model\n",
    "- low level control: tokenizer, model output\n",
    "- can extract hidden state from several layers, not just the last layer\n",
    "    - extract last hidden state easily\n",
    "    - extract several last hidden states, then average, before pooling to one array\n",
    "\n",
    "Feature-extraction `pipeline`:\n",
    "- extracted feature depends on the type of model (classifier), not just the task\n",
    "    - some models return logits\n",
    "- not much control over call of tokenizer\n",
    "- better to do inference, not extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "768-d embedding:\n",
    "- FinBert-ESG\n",
    "- sentence-roberta\n",
    "\n",
    "1024-d embedding:\n",
    "- financial Roberta\n",
    "- BART\n",
    "\n",
    "Model outputs are different, depends on model architecture\n",
    "- in encoder-decoder: 'encoder_hidden_state'\n",
    "- vanila attention: 'hidden_state'\n",
    "\n",
    "Model encoded_input from tokenizer are different:\n",
    "- some don't have 'token_type_ids'\n",
    "\n",
    "! Not compatible with our extraction workflow:\n",
    "- Flan-T5\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "torch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
