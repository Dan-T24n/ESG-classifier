{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Setup"]},{"cell_type":"markdown","metadata":{},"source":["## Load and install packages"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T22:34:20.586441Z","iopub.status.busy":"2023-06-21T22:34:20.585996Z","iopub.status.idle":"2023-06-21T22:34:20.597061Z","shell.execute_reply":"2023-06-21T22:34:20.596216Z","shell.execute_reply.started":"2023-06-21T22:34:20.586401Z"},"trusted":true},"outputs":[],"source":["%%capture\n","\n","# !pip install PyMuPDF\n","# !pip install icecream\n","# !pip install nltk\n","# !pip install ocrmypdf\n","# !pip install pytesseract\n","# !pip install pngquant\n","# !pip install fastparquet\n","# !pip install datasets\n","# !pip install fastparquet\n","# !pip install evaluate\n","# !pip install pycaret\n","# !pip install Umap\n","# !pip install git+https://github.com/huggingface/transformers.git\n","# !pip install git+https://github.com/huggingface/accelerate.git\n","# !pip install -U sentence-transformers\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T22:34:20.602866Z","iopub.status.busy":"2023-06-21T22:34:20.602249Z","iopub.status.idle":"2023-06-21T22:34:27.35597Z","shell.execute_reply":"2023-06-21T22:34:27.354997Z","shell.execute_reply.started":"2023-06-21T22:34:20.602834Z"},"trusted":true},"outputs":[],"source":["from tqdm.auto import tqdm\n","from pathlib import Path\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import pandas as pd \n","import fitz \n","from PIL import Image\n","import re\n","import pngquant\n","from unidecode import unidecode\n","import torch\n","import evaluate\n","from icecream import ic\n","\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{},"source":["## PDF Helper Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T22:34:27.35801Z","iopub.status.busy":"2023-06-21T22:34:27.357644Z","iopub.status.idle":"2023-06-21T22:34:27.375884Z","shell.execute_reply":"2023-06-21T22:34:27.374885Z","shell.execute_reply.started":"2023-06-21T22:34:27.357976Z"},"trusted":true},"outputs":[],"source":["def get_pdf(path):\n","    '''Reads a PDF file and returns a PdfDocument object.'''\n","    doc = fitz.open(path)\n","    return doc\n","\n","def render(page):\n","    '''Render a page to a PIL image.'''\n","    bitmap = page.render(\n","        scale = 1,    # 72dpi resolution\n","        rotation = 0, # no additional rotation\n","    )\n","    return bitmap.to_pil()\n","\n","# cleanup content using regex:\n","def clean_characters(text):\n","    text = unidecode(text) # convert to ascii\n","    text = re.sub(r\"[^a-zA-Z0-9.£$€']+\", ' ', text) # strategy 1: keep numbers\n","    # text = re.sub(r\"[^a-zA-Z:.£$€'\\n]+\", ' ', text) ## strategy 2: remove numbers and %\n","    text = ' '.join([w for w in text.split() if len(w)>1] )\n","    return text\n","\n","def extract_content_from_id(file_id: str) -> str :    \n","    # extract filename and page\n","    items = file_id.split('.')\n","    filename = '.'.join(items[:2])\n","    page_num = int(items[-1])-1\n","    \n","    # load pdf, select page, and extract its content\n","    filepath = os.path.join(report_path, filename) #path to the report\n","    doc = get_pdf(filepath) # load the pdf\n","    page = doc.load_page(page_num) # select the page from the pdf\n","    content = page.get_text(\"text\", sort = True, flags=fitz.TEXT_INHIBIT_SPACES) # extract the text from the page\n","    content = unidecode(content) # convert to ascii\n","    content = clean_characters(content) # replace unrecognized characters\n","    # should run OCR when needed, but too slow with this hardware, only OCR test-set\n","    return content\n","\n","def extract_content_OCR_test_set(file_id: str) -> str :\n","    '''Extract content from PDF  using OCR when needed: slower than extract_content_from_id'''\n","    # extract filename and page\n","    items = file_id.split('.')\n","    filename = '.'.join(items[:2])\n","    page_num = int(items[-1])-1\n","    \n","    # load pdf, select page, and extract its content\n","    filepath = os.path.join(report_path, filename) #path to the report\n","    doc = get_pdf(filepath) # load the pdf\n","    page = doc.load_page(page_num) # select the page from the pdf\n","    content = page.get_text(\"text\", sort = True, flags=fitz.TEXT_INHIBIT_SPACES) # extract the text from the page\n","    content = unidecode(content) # convert to ascii\n","    content = clean_characters(content) # replace unrecognized characters\n","    \n","    # #split the string to test parsing: OCR when needed\n","    # words = content.split()\n","    # avg_char = sum(len(word) for word in words)/len(words)\n","\n","    # # OCR if needed: check test below section OCR: too slow\n","    # if avg_char > 18:\n","    #     content = ocr_the_page(page)\n","        \n","    return content\n","\n","def render_content_from_id(file_id: str) -> str :    \n","    # extract filename and page\n","    items = file_id.split('.')\n","    filename = '.'.join(items[:2]) # reconstruct the report name\n","    page_num = int(items[-1])-1\n","    \n","    # load pdf, select page, and extract its content\n","    filepath = os.path.join(report_path, filename) #path to the report\n","    doc = get_pdf(filepath) # load the report pdf\n","    dpi = 150  # set the dpi\n","    mat = fitz.Matrix(dpi / 72, dpi / 72)  # sets zoom factor\n","    pix = doc[page_num].get_pixmap(matrix=mat)\n","    img_page = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n","    return img_page"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T22:34:27.37998Z","iopub.status.busy":"2023-06-21T22:34:27.379677Z","iopub.status.idle":"2023-06-21T22:34:27.501071Z","shell.execute_reply":"2023-06-21T22:34:27.500124Z","shell.execute_reply.started":"2023-06-21T22:34:27.379956Z"},"trusted":true},"outputs":[],"source":["# OCR Version 1: using PyMuPDF builtin ocr_the_page function\n","\n","import ocrmypdf\n","import sys\n","import io\n","\n","ocrmypdf.configure_logging(verbosity = -1)\n","\n","def ocr_the_page(page):\n","    \"\"\"Extract the text from passed-in PDF page.\"\"\"    \n","    src = page.parent  # the page's document\n","    doc = fitz.open()  # make temporary 1-pager\n","    doc.insert_pdf(src, from_page=page.number, to_page=page.number)\n","    pdfbytes = doc.tobytes()\n","    inbytes = io.BytesIO(pdfbytes)  # transform to BytesIO object\n","    outbytes = io.BytesIO()  # let ocrmypdf store its result pdf here\n","\n","    # run ocr and get result pdf as bytes\n","    ocrmypdf.ocr(\n","        inbytes,  # input 1-pager\n","        outbytes,  # ouput 1-pager\n","        language=\"eng\",  # modify as required e.g. (\"eng\", \"ger\")\n","        output_type=\"pdf\",  # only need simple PDF format\n","        # add more paramneters, e.g. to enforce OCR-ing, etc., e.g.\n","        force_ocr=True, \n","        # redo_ocr=True\n","        progress_bar=False,\n","        optimize=1,\n","    )\n","    \n","    # read result pdf as fitz document\n","    ocr_pdf = fitz.open(\"pdf\", outbytes.getvalue())  # read output as fitz PDF\n","    text = ocr_pdf[0].get_text()  # and extract text from the page\n","    return text  # return it\n","\n","# build function to extract page from file_id then call function `ocr_the_page`\n","def extract_content_from_id_ocr(file_id: str) -> str :\n","    # extract filename and page\n","    items = file_id.split('.')\n","    filename = '.'.join(items[:2])\n","    page_num = int(items[-1])-1\n","    \n","    # load pdf, select page, and extract its content\n","    filepath = os.path.join(report_path, filename) #path to the report\n","    doc = get_pdf(filepath) # load the pdf\n","    page = doc[page_num] # select the page from the pdf\n","    \n","    content = ocr_the_page(page)\n","    \n","    return content"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T22:34:27.50326Z","iopub.status.busy":"2023-06-21T22:34:27.502325Z","iopub.status.idle":"2023-06-21T22:34:27.511056Z","shell.execute_reply":"2023-06-21T22:34:27.510188Z","shell.execute_reply.started":"2023-06-21T22:34:27.503223Z"},"trusted":true},"outputs":[],"source":["# OCR Version 2: using PyMuPDF with Partial OCR\n","# doesn't work, require setting environment variable TESSDATA_PREFIX\n","\n","# def extract_content_from_id_ocr_partial(file_id: str) -> str :    \n","#     # extract filename and page\n","#     items = file_id.split('.')\n","#     filename = '.'.join(items[:2])\n","#     page_num = int(items[-1])-1\n","    \n","#     # load pdf, select page\n","#     filepath = os.path.join(report_path, filename) #path to the report\n","#     doc = get_pdf(filepath) # load the pdf\n","#     page = doc[page_num] # select the page from the pdf\n","    \n","#     # extract content\n","#     partial_tp = page.get_textpage_ocr(flags=0, full=False)\n","#     content = page.get_text(textpage=partial_tp, sort=True)\n","\n","#     return content"]},{"cell_type":"markdown","metadata":{},"source":["## Set path for folder\n","\n","- choose one of the following blocks: online if run directly on Kaggle, local if data downloaded"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T22:34:27.513329Z","iopub.status.busy":"2023-06-21T22:34:27.51232Z","iopub.status.idle":"2023-06-21T22:34:27.524644Z","shell.execute_reply":"2023-06-21T22:34:27.523574Z","shell.execute_reply.started":"2023-06-21T22:34:27.513296Z"},"trusted":true},"outputs":[],"source":["%%script echo skip\n","\n","# # Kaggle online path\n","\n","# basepath = \"/kaggle/input/oxml2023mlcases-esg-classifier\"\n","\n","# # build get path function, input folder name (as a list) and file name (optional), return the path\n","# def get_path(folder_name: list, file_name=None):\n","#     path = Path(basepath)\n","#     for folder in folder_name:\n","#         path = path / folder\n","#     if file_name:\n","#         path = path / file_name\n","#     return path\n","\n","\n","# # report path\n","# report_path = get_path([\"data\", \"reports\"])\n","\n","# # label path\n","# label_path = get_path([\"data\"], \"labels.csv\")\n","\n","# # output path\n","# output_path = '/kaggle/working/'\n","\n","# submit_path = get_path([], \"sample_submission.csv\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T22:34:27.528083Z","iopub.status.busy":"2023-06-21T22:34:27.527298Z","iopub.status.idle":"2023-06-21T22:34:27.535459Z","shell.execute_reply":"2023-06-21T22:34:27.534541Z","shell.execute_reply.started":"2023-06-21T22:34:27.528049Z"},"trusted":true},"outputs":[],"source":["# local data path \n","\n","# current working directory, go up 1 level\n","basepath = Path.cwd().parents[0]\n","\n","# build get path function, input folder name (as a list) and file name (optional), return the path\n","def get_path(folder_name: list, file_name=None):\n","    path = Path(basepath)\n","    for folder in folder_name:\n","        path = path / folder\n","    if file_name:\n","        path = path / file_name\n","    return path\n","\n","# report path\n","report_path = get_path([\"data\", \"reports\"])\n","\n","# label path\n","label_path = get_path([\"data\"], \"labels.csv\")\n","\n","# output path\n","output_path = get_path([], \"output\")\n","\n","# submit sample path\n","submit_path = get_path([], \"sample_submission.csv\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T22:34:27.538806Z","iopub.status.busy":"2023-06-21T22:34:27.538547Z","iopub.status.idle":"2023-06-21T22:34:27.547017Z","shell.execute_reply":"2023-06-21T22:34:27.546038Z","shell.execute_reply.started":"2023-06-21T22:34:27.538783Z"},"trusted":true},"outputs":[],"source":["print(\"Report path: \", report_path)\n","print(\"Label path: \", label_path)\n","print(\"Output path: \", output_path)\n","print(\"Submit path: \", submit_path)"]},{"cell_type":"markdown","metadata":{},"source":["## Test OCR"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T22:34:27.548846Z","iopub.status.busy":"2023-06-21T22:34:27.548438Z","iopub.status.idle":"2023-06-21T22:34:27.566736Z","shell.execute_reply":"2023-06-21T22:34:27.566011Z","shell.execute_reply.started":"2023-06-21T22:34:27.548811Z"},"trusted":true},"outputs":[],"source":["%%script echo skip\n","\n","# load file id\n","file_id = 'report_1611.pdf.16'\n","\n","# render the page\n","render_content_from_id(file_id)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T22:34:27.567985Z","iopub.status.busy":"2023-06-21T22:34:27.56766Z","iopub.status.idle":"2023-06-21T22:34:27.580518Z","shell.execute_reply":"2023-06-21T22:34:27.579618Z","shell.execute_reply.started":"2023-06-21T22:34:27.567947Z"},"trusted":true},"outputs":[],"source":["%%script echo skip\n","\n","# extract content from the page\n","test_no_ocr = extract_content_from_id(file_id)\n","test_with_ocr = extract_content_from_id_ocr(file_id)\n","# test_partial_ocr = extract_content_from_id_ocr_partial(file_id): not working, can't set ENV var\n","\n","print(f'*** start testing:[{file_id}] *** \\n')\n","print('\\n *** no OCR *** \\n')\n","print(test_no_ocr)\n","print('\\n *** with OCR *** \\n')\n","print(test_with_ocr)\n","# print('\\n *** partial OCR *** \\n')\n","# print(test_partial_ocr)\n","# print(f'*** end test *** \\n\\n')"]},{"cell_type":"markdown","metadata":{},"source":["# Parse PDF and process text"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T22:34:27.58181Z","iopub.status.busy":"2023-06-21T22:34:27.581401Z","iopub.status.idle":"2023-06-21T22:34:27.600962Z","shell.execute_reply":"2023-06-21T22:34:27.599819Z","shell.execute_reply.started":"2023-06-21T22:34:27.581777Z"},"trusted":true},"outputs":[],"source":["# %%script echo skipping\n","\n","# read label file to get id and class\n","df = pd.read_csv(label_path)\n","\n","# check duplicates id\n","print(\"Number of duplicated id: \", df.duplicated(subset=['id']).sum())\n","\n","# slice duplicated id, all entries with duplicated id\n","df_duplicate = df[df.duplicated(subset=['id'], keep=False)].sort_values(by=['id'])\n","df_duplicate # duplicates have conflicting labels, due to nature of the pages\n","\n","# drop the duplicates (keep the first entry)\n","df = df.drop_duplicates(subset=['id'], keep='first').sort_values(by=['id'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T22:34:27.602731Z","iopub.status.busy":"2023-06-21T22:34:27.602386Z","iopub.status.idle":"2023-06-21T22:35:26.685091Z","shell.execute_reply":"2023-06-21T22:35:26.684139Z","shell.execute_reply.started":"2023-06-21T22:34:27.6027Z"},"trusted":true},"outputs":[],"source":["# %%script echo skipping\n","\n","# make wrapper function to extract content and save to df\n","def get_content(df):\n","    \n","    contents = []\n","    \n","    for idx, row in tqdm(df.iterrows(), total=len(df)):\n","        # get file id\n","        file_id = row['id']\n","        # extract content from \n","        content = extract_content_from_id(file_id)\n","        contents.append(content)\n","    \n","    df['content'] = contents\n","    \n","    return df\n","\n","# apply function to parse PDF\n","df = get_content(df)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T22:35:26.691848Z","iopub.status.busy":"2023-06-21T22:35:26.690915Z","iopub.status.idle":"2023-06-21T22:36:22.88011Z","shell.execute_reply":"2023-06-21T22:36:22.879145Z","shell.execute_reply.started":"2023-06-21T22:35:26.691812Z"},"trusted":true},"outputs":[],"source":["%%script echo skipping\n","\n","# # make wrapper function to extract content and save to df\n","# def get_content_test_set(df):\n","    \n","#     contents = []\n","    \n","#     for idx, row in tqdm(df.iterrows(), total=len(df)):\n","#         # get file id\n","#         file_id = row['id']\n","#         # extract content from \n","#         content = extract_content_from_id_test_set(file_id)\n","#         contents.append(content)\n","    \n","#     df['content'] = contents\n","    \n","#     return df\n","\n","# # apply function to parse PDF\n","# df = get_content(df)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T22:36:22.882175Z","iopub.status.busy":"2023-06-21T22:36:22.881711Z","iopub.status.idle":"2023-06-21T22:36:22.898687Z","shell.execute_reply":"2023-06-21T22:36:22.897775Z","shell.execute_reply.started":"2023-06-21T22:36:22.882122Z"},"trusted":true},"outputs":[],"source":["# %%script echo skipping\n","\n","# create a new column with the report name\n","df['report_name'] = df['id'].apply(lambda x: x.split('.')[0])\n","\n","# creat a new column with the page number\n","df['page_num'] = df['id'].apply(lambda x: x.split('.')[-1])\n","\n","# reorder columns\n","df = df[['id','report_name', 'page_num', 'class', 'content']]\n","\n","# dictionary mapping class label to numeric\n","map_label = {'other': 0, 'environmental': 1, 'social': 2, 'governance': 3}\n","\n","# use the dictionary to replace the class label\n","df['target'] = df['class'].map(map_label)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T22:36:22.901944Z","iopub.status.busy":"2023-06-21T22:36:22.901591Z","iopub.status.idle":"2023-06-21T22:36:23.313385Z","shell.execute_reply":"2023-06-21T22:36:23.312341Z","shell.execute_reply.started":"2023-06-21T22:36:22.901908Z"},"trusted":true},"outputs":[],"source":["# %%script echo skipping\n","\n","# function to extract starting part and ending part of text content\n","def extract_block(text, num_token):\n","    # calculate the number of characters\n","    block_length = 5*num_token # assuming 1 token has 5 characters on average\n","    \n","    # extract the start, and end blocks\n","    start_block = text[:block_length]\n","    end_block = text[-block_length:]\n","\n","    return start_block, end_block\n","\n","\n","# build wrapper function on a dataframe, using the extract_block function above\n","def make_short_content(df, num_token):\n","    \n","    # make one new columns: join start and end blocks\n","    df['short_content'] = ''\n","\n","    # iterate over each row, extract blocks and assign to new columns\n","    for idx, row in tqdm(df.iterrows(), total=len(df)):\n","        content = row['content']\n","        start_block, end_block = extract_block(content, num_token)\n","        \n","        # joint the start and end blocks\n","        short_content = '.'.join([start_block, end_block])\n","        \n","        # assign to the new column\n","        df.loc[idx, 'short_content'] = short_content\n","        \n","    return df\n","\n","# transform content to shorten content\n","df = make_short_content(df,150)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T22:36:23.31535Z","iopub.status.busy":"2023-06-21T22:36:23.31473Z","iopub.status.idle":"2023-06-21T22:36:23.427111Z","shell.execute_reply":"2023-06-21T22:36:23.42604Z","shell.execute_reply.started":"2023-06-21T22:36:23.315314Z"},"trusted":true},"outputs":[],"source":["# %%script echo skipping\n","\n","# save local data file\n","path = Path(output_path)\n","file_data = path / 'data_clean.parquet'\n","print(file_data)\n","\n","# save df to parquet\n","df.to_parquet(file_data, engine='fastparquet')\n"]},{"cell_type":"markdown","metadata":{},"source":["# Fine-tuning: start here"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T22:36:23.428834Z","iopub.status.busy":"2023-06-21T22:36:23.428469Z","iopub.status.idle":"2023-06-21T22:36:23.45916Z","shell.execute_reply":"2023-06-21T22:36:23.458144Z","shell.execute_reply.started":"2023-06-21T22:36:23.428797Z"},"trusted":true},"outputs":[],"source":["# Load clean data from local storage\n","\n","# local data file\n","path = Path(output_path)\n","file_data = path / 'data_clean.parquet'\n","print(file_data)\n","\n","# reload df from parquet\n","df = pd.read_parquet(file_data, engine='fastparquet')"]},{"cell_type":"markdown","metadata":{},"source":["## Split report\n","\n","- objective: unique reports, pages, classes, all are roughly the same in both sets"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T22:36:23.461136Z","iopub.status.busy":"2023-06-21T22:36:23.460782Z","iopub.status.idle":"2023-06-21T22:36:23.474191Z","shell.execute_reply":"2023-06-21T22:36:23.473188Z","shell.execute_reply.started":"2023-06-21T22:36:23.461103Z"},"trusted":true},"outputs":[],"source":["# convert column page_num to int\n","df['page_num'] = df['page_num'].astype(int)\n","\n","# get unique report names along with total number of pages\n","num_pages_df = df.groupby('report_name')[['page_num']].nunique()\n","\n","# make a new column: value H for reports with more than 20 pages, L otherwise\n","median = num_pages_df['page_num'].median()\n","num_pages_df['pages'] = np.where(num_pages_df['page_num'] > median, 'H', 'L')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T22:36:23.476124Z","iopub.status.busy":"2023-06-21T22:36:23.475791Z","iopub.status.idle":"2023-06-21T22:36:23.487145Z","shell.execute_reply":"2023-06-21T22:36:23.486202Z","shell.execute_reply.started":"2023-06-21T22:36:23.476093Z"},"trusted":true},"outputs":[],"source":["# get unique report names along average target value\n","target_df = df.groupby('report_name')[['target']].mean()\n","\n","# discretize the avg target value to integer: try to distribute reports evenly among classes\n","# not really an issue since labels are distribured evenly among non-zero classes\n","bins = [0, 0.99, 1.5, 2,3] # boundaries for the bins\n","labels = [0, 1, 2, 3]   # labels for the bins\n","target_df['avg_target'] = pd.cut(x = target_df['target'], bins = bins, labels = labels, include_lowest = True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T22:36:23.489869Z","iopub.status.busy":"2023-06-21T22:36:23.489093Z","iopub.status.idle":"2023-06-21T22:36:23.503645Z","shell.execute_reply":"2023-06-21T22:36:23.50269Z","shell.execute_reply.started":"2023-06-21T22:36:23.489832Z"},"trusted":true},"outputs":[],"source":["# merge the two dataframes using report_name as key, create new index\n","df_report = pd.merge(num_pages_df, target_df, on='report_name').reset_index()\n","\n","# when the column avg_target is not zero, joint the pages and avg_arget columns as string\n","df_report['split_label'] = np.where(df_report['avg_target'] == 0, 0, df_report['avg_target'].astype(str) + df_report['pages'])\n","\n","# convert split_label to string\n","df_report['split_label'] = df_report['split_label'].astype(str)\n","\n","# keep only report_name and split_label columns\n","df_report = df_report[['report_name','page_num','split_label']]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T22:39:33.879208Z","iopub.status.busy":"2023-06-21T22:39:33.878768Z","iopub.status.idle":"2023-06-21T22:39:33.891676Z","shell.execute_reply":"2023-06-21T22:39:33.890515Z","shell.execute_reply.started":"2023-06-21T22:39:33.879161Z"},"trusted":true},"outputs":[],"source":["# import sklearn train test split stratify\n","from sklearn.model_selection import train_test_split\n","\n","# use sklearn to split with stratify on target split column\n","# set test_size minimal when done fine-tuning: use max amount of data possible\n","train_report, test_report = train_test_split(df_report, test_size=0.15, stratify=df_report['split_label'], random_state=42)\n","\n","# print total unique report name in each set\n","print(\"Total unique report name in train set: \", len(train_report['report_name'].unique()))\n","print(\"Total unique report name in test set: \", len(test_report['report_name'].unique()))\n","\n","# print total pages in each set\n","print(\"Total pages in train set: \", train_report['page_num'].sum())\n","print(\"Total pages in test set: \", test_report['page_num'].sum())"]},{"cell_type":"markdown","metadata":{},"source":["## Prepare data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T22:39:33.894233Z","iopub.status.busy":"2023-06-21T22:39:33.89369Z","iopub.status.idle":"2023-06-21T22:39:33.907361Z","shell.execute_reply":"2023-06-21T22:39:33.906337Z","shell.execute_reply.started":"2023-06-21T22:39:33.8942Z"},"trusted":true},"outputs":[],"source":["# slice the df to keep only the train report names: total pages will match\n","train_df = df[df['report_name'].isin(train_report['report_name'])]\n","\n","# slice the df to keep only the test report names\n","test_df = df[df['report_name'].isin(test_report['report_name'])]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T22:39:33.910095Z","iopub.status.busy":"2023-06-21T22:39:33.909296Z","iopub.status.idle":"2023-06-21T22:39:33.93094Z","shell.execute_reply":"2023-06-21T22:39:33.929967Z","shell.execute_reply.started":"2023-06-21T22:39:33.910061Z"},"trusted":true},"outputs":[],"source":["import datasets\n","from datasets import Dataset, DatasetDict\n","\n","# slice short content and label from train_df, rename short_content to text\n","train_df = train_df[['id','short_content', 'target']]\n","train_df = train_df.rename(columns={'short_content': 'text', 'target': 'label'})\n","\n","# convert to huggingface dataset\n","train = Dataset.from_pandas(train_df, preserve_index=False)\n","\n","# get short content and label from test_df, rename short_content to text\n","test_df = test_df[['id','short_content', 'target']]\n","test_df = test_df.rename(columns={'short_content': 'text', 'target': 'label'})\n","\n","# convert to huggingface dataset\n","validation = Dataset.from_pandas(test_df, preserve_index=False)\n","\n","\n","print(train)\n","print(validation)"]},{"cell_type":"markdown","metadata":{},"source":["## Set up pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T22:39:33.932658Z","iopub.status.busy":"2023-06-21T22:39:33.932113Z","iopub.status.idle":"2023-06-21T22:39:33.937498Z","shell.execute_reply":"2023-06-21T22:39:33.936514Z","shell.execute_reply.started":"2023-06-21T22:39:33.932628Z"},"trusted":true},"outputs":[],"source":["# list of pre-trained models\n","finbert_esg = 'yiyanghkust/finbert-esg'\n","sentence_roberta = 'sentence-transformers/all-distilroberta-v1'\n","fin_roberta = 'soleimanian/financial-roberta-large-sentiment'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T22:39:33.940414Z","iopub.status.busy":"2023-06-21T22:39:33.939669Z","iopub.status.idle":"2023-06-21T22:39:33.948759Z","shell.execute_reply":"2023-06-21T22:39:33.947805Z","shell.execute_reply.started":"2023-06-21T22:39:33.94038Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification \n","from transformers import TrainingArguments, Trainer , DataCollatorWithPadding\n","from transformers import pipeline\n","from transformers.pipelines.pt_utils import KeyDataset\n","import evaluate\n","\n","# from transformers import logging\n","# logging.set_verbosity_error()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T22:39:33.950842Z","iopub.status.busy":"2023-06-21T22:39:33.950016Z","iopub.status.idle":"2023-06-21T22:39:33.962861Z","shell.execute_reply":"2023-06-21T22:39:33.961912Z","shell.execute_reply.started":"2023-06-21T22:39:33.950809Z"},"trusted":true},"outputs":[],"source":["os.environ[\"WANDB_DISABLED\"] = \"true\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T22:39:33.964021Z","iopub.status.busy":"2023-06-21T22:39:33.963774Z","iopub.status.idle":"2023-06-21T22:39:36.762192Z","shell.execute_reply":"2023-06-21T22:39:36.761206Z","shell.execute_reply.started":"2023-06-21T22:39:33.963999Z"},"trusted":true},"outputs":[],"source":["# set up tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(finbert_esg)\n","\n","# tokenize function\n","def tokenize_function(example):\n","    return tokenizer(example['text'], padding=\"max_length\", max_length=256, truncation=True)\n","\n","# map tokenize function to train and test dataset\n","tokenized_train_dataset = train.map(tokenize_function, batched=True)\n","tokenized_test_dataset = validation.map(tokenize_function, batched=True)\n","\n","# define data collator (stick batches together), use later in trainer\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","print(tokenized_train_dataset)\n","print(tokenized_test_dataset)"]},{"cell_type":"markdown","metadata":{},"source":["## Fine-tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T22:39:36.765997Z","iopub.status.busy":"2023-06-21T22:39:36.764961Z","iopub.status.idle":"2023-06-21T22:39:38.969444Z","shell.execute_reply":"2023-06-21T22:39:38.968474Z","shell.execute_reply.started":"2023-06-21T22:39:36.765958Z"},"trusted":true},"outputs":[],"source":["# Fine-tuning if needed\n","\n","# set up model\n","model = AutoModelForSequenceClassification.from_pretrained(finbert_esg, num_labels=4)\n","\n","\n","# set up training arguments : output directory, evaluation strategy\n","# # for Kaggle\n","# training_args = TrainingArguments(output_dir=output_path, \n","#                                   evaluation_strategy=\"epoch\",\n","#                                   save_strategy=\"epoch\",\n","#                                   save_total_limit = 1,\n","#                                   overwrite_output_dir = True,\n","#                                   logging_steps=200,\n","#                                   learning_rate= 0.0001,\n","#                                   per_device_train_batch_size=16,\n","#                                   per_device_eval_batch_size=16,\n","#                                   num_train_epochs=3,\n","#                                   load_best_model_at_end=True,\n","#                                   weight_decay=0.01,\n","#                                   metric_for_best_model='eval_loss'\n","#                                  ) \n","\n","# for M1 Mac: doesn't work\n","training_args = TrainingArguments(output_dir=output_path, evaluation_strategy=\"epoch\", use_mps_device=True)  \n","\n","def compute_metrics(eval_preds):   # compute accuracy and f1-score\n","    f1_metric = evaluate.load(\"f1\")\n","    logits, labels = eval_preds\n","    predictions = np.argmax(logits, axis=-1)\n","    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"micro\")\n","    return f1\n","\n","# Trainer object has automated training loop: can write our custom training loop in PyTorch native\n","trainer = Trainer( \n","    model,\n","    training_args,\n","    train_dataset=tokenized_train_dataset,\n","    eval_dataset=tokenized_test_dataset,\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T22:39:38.972201Z","iopub.status.busy":"2023-06-21T22:39:38.97148Z","iopub.status.idle":"2023-06-21T22:46:20.732547Z","shell.execute_reply":"2023-06-21T22:46:20.731291Z","shell.execute_reply.started":"2023-06-21T22:39:38.972143Z"},"trusted":true},"outputs":[],"source":["# # %%script echo skip\n","trainer.train()  # starts fine-tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# os.makedirs('/kaggle/working/model_checkpoint')\n","\n","trainer.save_model('/kaggle/working/model_checkpoint')"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2023-06-21T04:27:42.829311Z","iopub.status.busy":"2023-06-21T04:27:42.828596Z","iopub.status.idle":"2023-06-21T04:27:42.835146Z","shell.execute_reply":"2023-06-21T04:27:42.832906Z","shell.execute_reply.started":"2023-06-21T04:27:42.829276Z"}},"source":["# Inference on test-set"]},{"cell_type":"markdown","metadata":{},"source":["## Get test data and process content"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T22:46:20.746366Z","iopub.status.busy":"2023-06-21T22:46:20.744878Z","iopub.status.idle":"2023-06-21T22:46:20.767142Z","shell.execute_reply":"2023-06-21T22:46:20.766184Z","shell.execute_reply.started":"2023-06-21T22:46:20.745807Z"},"trusted":true},"outputs":[],"source":["# read sample submission file\n","df_submit = pd.read_csv(\"/kaggle/input/oxml2023mlcases-esg-classifier/sample_submission.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T22:47:57.303516Z","iopub.status.busy":"2023-06-21T22:47:57.303102Z","iopub.status.idle":"2023-06-21T22:54:51.295396Z","shell.execute_reply":"2023-06-21T22:54:51.29423Z","shell.execute_reply.started":"2023-06-21T22:47:57.303485Z"},"trusted":true},"outputs":[],"source":["# get content from test set\n","df_submit = get_content_test_set(df_submit)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T22:55:20.659044Z","iopub.status.busy":"2023-06-21T22:55:20.658653Z","iopub.status.idle":"2023-06-21T22:55:20.749513Z","shell.execute_reply":"2023-06-21T22:55:20.748475Z","shell.execute_reply.started":"2023-06-21T22:55:20.65901Z"},"trusted":true},"outputs":[],"source":["# get shorten content from full text content\n","df_submit = make_short_content(df_submit, 100)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T22:55:27.931651Z","iopub.status.busy":"2023-06-21T22:55:27.931269Z","iopub.status.idle":"2023-06-21T22:55:27.939424Z","shell.execute_reply":"2023-06-21T22:55:27.938333Z","shell.execute_reply.started":"2023-06-21T22:55:27.931619Z"},"trusted":true},"outputs":[],"source":["# extract one ranndom row\n","sample_row = df_submit.sample(1).iloc[0]\n","\n","# print content vs. short content\n","print(sample_row['content'])\n","print(f'\\n\\n\\n')\n","print(sample_row['short_content'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T22:55:38.689053Z","iopub.status.busy":"2023-06-21T22:55:38.688048Z","iopub.status.idle":"2023-06-21T22:55:38.694645Z","shell.execute_reply":"2023-06-21T22:55:38.693693Z","shell.execute_reply.started":"2023-06-21T22:55:38.689018Z"},"trusted":true},"outputs":[],"source":["# keep short content \n","df_submit = df_submit[['id','short_content']]"]},{"cell_type":"markdown","metadata":{},"source":["## Make prediction with fine-tuned model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T22:55:27.952571Z","iopub.status.busy":"2023-06-21T22:55:27.951494Z","iopub.status.idle":"2023-06-21T22:55:27.960741Z","shell.execute_reply":"2023-06-21T22:55:27.959847Z","shell.execute_reply.started":"2023-06-21T22:55:27.952537Z"},"trusted":true},"outputs":[],"source":["# %%script echo skip\n","\n","# # # get prediction : choose this one or below\n","\n","# df_submit['class'] = ''\n","\n","# # iteratively get prediction for each row in df_submit\n","# for index, row in df_submit.iterrows():\n","        \n","#         # extract content from row\n","#         text = row['short_content']\n","#         # tokenize content\n","#         encoded_input = tokenizer(text, padding=\"max_length\", max_length=512, truncation=True, return_tensors='pt')\n","#         # get prediction\n","#         output = model(**encoded_input)\n","        \n","#         # print(output)\n","#         pred = int(torch.argmax(output.logits, axis=1).detach())\n","#         # print(f'Predicted label is: {pred}')\n","#         df_submit.iloc[index]['class'] = pred\n","\n","    \n","# # dictionary mapping class label to numeric\n","# map_label = {'other': 0, 'environmental': 1, 'social': 2, 'governance': 3}\n","\n","# # reverse the dictionary\n","# reverse_map_label = {v: k for k, v in map_label.items()}\n","\n","# # make a copy of submission df\n","# submission = df_submit.copy()\n","\n","# # drop column content\n","# submission.drop(columns=['short_content'], inplace=True)\n","\n","# # map numeric label to class label\n","# submission['class'] = submission['class'].map(reverse_map_label)\n","\n","# submission"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T22:55:27.96481Z","iopub.status.busy":"2023-06-21T22:55:27.964486Z","iopub.status.idle":"2023-06-21T22:55:38.672066Z","shell.execute_reply":"2023-06-21T22:55:38.671046Z","shell.execute_reply.started":"2023-06-21T22:55:27.964776Z"},"trusted":true},"outputs":[],"source":["# %%script echo skip \n","\n","# works faster, but harder to low-level control\n","\n","# set up pipeline\n","clf = pipeline(\"text-classification\",model=model,tokenizer=tokenizer,device=0)\n","tokenizer_kwargs = {'padding': 'max_length','truncation':True,'max_length':512}\n","\n","# get prediction\n","\n","df_submit['class'] = ''\n","\n","# iteratively get prediction for each row in df_submit\n","for index, row in df_submit.iterrows():\n","        \n","        # extract content from row\n","        text = row['short_content']\n","\n","        # get prediction\n","        pred = clf(text,**tokenizer_kwargs)\n","        # print(pred[0]['label'])\n","        \n","        # append prediction class label\n","        df_submit.iloc[index]['class'] = pred[0]['label']\n","        \n","# make a copy of submission df\n","submission = df_submit[['id','class']].copy()\n","\n","# dictionary mapping class label to numeric\n","map_label = {'other': 'None', 'environmental': 'Environmental', 'social': 'Social', 'governance': 'Governance'}\n","\n","# reverse the dictionary\n","reverse_map_label = {v: k for k, v in map_label.items()}\n","\n","# map numeric label to class label\n","submission['class'] = submission['class'].map(reverse_map_label)\n","\n","# display\n","submission"]},{"cell_type":"markdown","metadata":{},"source":["## Submit file"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T22:55:38.674097Z","iopub.status.busy":"2023-06-21T22:55:38.673535Z","iopub.status.idle":"2023-06-21T22:55:38.685878Z","shell.execute_reply":"2023-06-21T22:55:38.684859Z","shell.execute_reply.started":"2023-06-21T22:55:38.674061Z"},"trusted":true},"outputs":[],"source":["# convert to csv\n","submission.to_csv('/kaggle/working/submission.csv', index=False)\n","\n","submission.to_csv('submission.csv', index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.15"}},"nbformat":4,"nbformat_minor":4}
