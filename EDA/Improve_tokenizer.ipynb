{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install -U pypdfium2\n",
    "# !pip install PyMuPDF\n",
    "# !pip install -U nltk\n",
    "# !pip install ocrmypdf -U\n",
    "# !brew install tesseract\n",
    "# !pip install pytesseract\n",
    "# !pip install -U pngquant\n",
    "# !pip install keras\n",
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T15:45:06.273642Z",
     "iopub.status.busy": "2023-06-02T15:45:06.273223Z",
     "iopub.status.idle": "2023-06-02T15:45:06.282024Z",
     "shell.execute_reply": "2023-06-02T15:45:06.281125Z",
     "shell.execute_reply.started": "2023-06-02T15:45:06.273601Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd \n",
    "import pypdfium2 as pdfium\n",
    "from icecream import ic\n",
    "import fitz \n",
    "from PIL import Image\n",
    "import re\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T15:45:06.283674Z",
     "iopub.status.busy": "2023-06-02T15:45:06.283375Z",
     "iopub.status.idle": "2023-06-02T15:45:06.299613Z",
     "shell.execute_reply": "2023-06-02T15:45:06.298548Z",
     "shell.execute_reply.started": "2023-06-02T15:45:06.283648Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_pdf(path):\n",
    "    '''Reads a PDF file and returns a PdfDocument object.'''\n",
    "    doc = fitz.open(path)\n",
    "    return doc\n",
    "\n",
    "def render(page):\n",
    "    '''Render a page to a PIL image.'''\n",
    "    bitmap = page.render(\n",
    "        scale = 1,    # 72dpi resolution\n",
    "        rotation = 0, # no additional rotation\n",
    "    )\n",
    "    return bitmap.to_pil()\n",
    "\n",
    "# text processing: \n",
    "# replace unrecognized characters with space using regex\n",
    "# breaking hyperlinks\n",
    "def replace_unrecognized_characters(text):\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\n:.?!$â‚¬/']+\", ' ', text)\n",
    "    return text\n",
    "\n",
    "def extract_content_from_id(file_id: str) -> str :    \n",
    "    # extract filename and page\n",
    "    items = file_id.split('.')\n",
    "    filename = '.'.join(items[:2])\n",
    "    page_num = int(items[-1])-1\n",
    "    \n",
    "    # load pdf, select page, and extract its content\n",
    "    filepath = os.path.join(report_path, filename) #path to the report\n",
    "    doc = get_pdf(filepath) # load the pdf\n",
    "    page = doc.load_page(page_num) # select the page from the pdf\n",
    "    content = page.get_text() # extract the text from the page\n",
    "    content = replace_unrecognized_characters(content) # replace unrecognized characters\n",
    "    return content\n",
    "\n",
    "def render_content_from_id(file_id: str) -> str :    \n",
    "    # extract filename and page\n",
    "    items = file_id.split('.')\n",
    "    filename = '.'.join(items[:2]) # reconstruct the report name\n",
    "    page_num = int(items[-1])-1\n",
    "    \n",
    "    # load pdf, select page, and extract its content\n",
    "    filepath = os.path.join(report_path, filename) #path to the report\n",
    "    doc = get_pdf(filepath) # load the report pdf\n",
    "    dpi = 100   # set the dpi\n",
    "    mat = fitz.Matrix(dpi / 72, dpi / 72)  # sets zoom factor\n",
    "    pix = doc[page_num].get_pixmap(matrix=mat)\n",
    "    img_page = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "    return img_page"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set path for folder\n",
    "\n",
    "- choose one of the following blocks: online if run directly on Kaggle, local if data downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local data path using pathlib\n",
    "\n",
    "# main folder\n",
    "basepath = \"/Users/macmini/Desktop/GoogleDrive/OxML_2023/code/Kaggle\"\n",
    "\n",
    "# build get path function, input folder name (as a list) and file name (optional), return the path\n",
    "def get_path(folder_name: list, file_name=None):\n",
    "    path = Path(basepath)\n",
    "    for folder in folder_name:\n",
    "        path = path / folder\n",
    "    if file_name:\n",
    "        path = path / file_name\n",
    "    return path\n",
    "\n",
    "# report path\n",
    "report_path = get_path([\"data\", \"reports\"])\n",
    "\n",
    "# label path\n",
    "label_path = get_path([\"data\"], \"labels.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report path:  /Users/macmini/Desktop/GoogleDrive/OxML_2023/code/Kaggle/data/reports\n",
      "Label path:  /Users/macmini/Desktop/GoogleDrive/OxML_2023/code/Kaggle/data/labels.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Report path: \", report_path)\n",
    "print(\"Label path: \", label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of sample csv files\n",
    "files = ['sample_few_token.csv', 'sample_long_token.csv', 'sample_short_token.csv']\n",
    "\n",
    "# load sample file from csv\n",
    "sample_few_token = pd.read_csv(get_path([\"process\"], files[0]), index_col=None)\n",
    "sample_long_token = pd.read_csv(get_path([\"process\"], files[1]), index_col=None)\n",
    "sample_short_token = pd.read_csv(get_path([\"process\"], files[2]), index_col=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insight for cleaning and tokenizer\n",
    "\n",
    "- good text processing solves long tokens, OCR parsing it not needed\n",
    "  - hyperlinks should work\n",
    "  - footnotes should work\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "- OCR may be useful for tables and charts\n",
    "  - quite slow in costly\n",
    "  - deploy unless really needed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improve tokenizers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex cleaning\n",
    "- clean text content before tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unrecognized characters: replace with space using regex\n",
    "# breaking hyperlinks: exclude / from text\n",
    "\n",
    "def replace_unrecognized_characters(text):\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\n;:,.?!]+\", ' ', text)\n",
    "    return text\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build text processing functions\n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id:  report_1826.pdf.55\n",
      "sentences *** \n",
      "  0    Further articles on the subject at:\\naudi.com ...\n",
      "dtype: object\n",
      "\n",
      " *** \n",
      "\n",
      "id:  report_1829.pdf.157\n",
      "sentences *** \n",
      "  0     \\nD Consolidated Financial Statements \\n \\n \\...\n",
      "1    r.l., Luxembourg \\n100.0 \\nAllianz Pension Con...\n",
      "2    Invest \\nKG, Berlin \\n50.0 3 \\nRoland Holding ...\n",
      "3    r.l., \\nCasablanca \\n100.0 \\nSeine GmbH, Munic...\n",
      "dtype: object\n",
      "\n",
      " *** \n",
      "\n",
      "id:  report_1825.pdf.142\n",
      "sentences *** \n",
      "  0    CORPORATE GOVERNANCE \\n3.2 EXECUTIVE COMPENSAT...\n",
      "1    b Employees other than corporate of icers at g...\n",
      "2    In the table above all dates that are indicate...\n",
      "3    140 \\n \\n \\nI REGISTRATION DOCUMENT ANNUAL REP...\n",
      "dtype: object\n",
      "\n",
      " *** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get one id from each sample df and concatenate them into a list\n",
    "ids = [sample_few_token['id'][0], sample_long_token['id'][0], sample_short_token['id'][0]]\n",
    "\n",
    "# test sentence tokenizer on each id\n",
    "for id in ids:\n",
    "    print(\"id: \", id)\n",
    "    content = extract_content_from_id(id)\n",
    "    sentences = sent_tokenize(content)\n",
    "    # convert list to series\n",
    "    sentences = pd.Series(sentences)\n",
    "    # count the number of sentences in the content\n",
    "    print(\"sentences *** \\n \", sentences)\n",
    "    print('\\n *** \\n')\n",
    "\n",
    "# sentence tokenizer: can't deal with multiple linebreaks, tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a function to preprocess text: tokenize, remove stopwords and punctuation, lemmatize\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "def tokenize_text(text):\n",
    "  \n",
    "  # Tokenize the text\n",
    "  tokens = word_tokenize(text.lower())\n",
    "  \n",
    "  # filter stopwords and punctuation\n",
    "  keep_tokens = [token for token in tokens if token not in stopwords.words('english') and token not in string.punctuation]\n",
    "  \n",
    "  # Remove special characters\n",
    "  tokens = [re.sub(r\"[^a-zA-Z]+\", '', token) for token in tokens]\n",
    "  \n",
    "  # Remove empty strings\n",
    "  tokens = list(filter(None, tokens))\n",
    "\n",
    "  # Lemmatize words\n",
    "  tokens = [lem.lemmatize(token, \"v\") for token in tokens]\n",
    "  \n",
    "  # Remove token that are less than 1 character: cleaning up\n",
    "  tokens = [token for token in tokens if len(token) > 1]\n",
    "\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull all documents into a list\n",
    "docs = [extract_content_from_id(id) for id in ids]\n",
    "\n",
    "# tokenize each doc then concatenate them into a single list\n",
    "tokens = [tokenize_text(doc) for doc in docs]\n",
    "tokens = [item for sublist in tokens for item in sublist]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# # create the tokenizer\n",
    "# tokenizer = Tokenizer()\n",
    "\n",
    "# # fit the tokenizer on the documents\n",
    "# tokenizer.fit_on_texts(docs)\n",
    "\n",
    "# # summarize info on the tokenizer\n",
    "# print(\"word counts: \", tokenizer.word_counts)\n",
    "# print(\"document count: \", tokenizer.document_count)\n",
    "# print(\"word index: \", tokenizer.word_index)\n",
    "# print(\"word docs: \", tokenizer.word_docs)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
